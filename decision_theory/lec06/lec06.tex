\documentclass[twoside]{article}

\usepackage{amsmath,amsthm,amssymb,epsfig}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem*{exmp}{Example}
\newtheorem*{problem}{Problem}
\newtheorem*{claim}{Claim}

\theoremstyle{remark}
\newtheorem*{rem}{Remark}
\newtheorem*{note}{Note}
\newtheorem*{exer}{Exercise}

\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

\DeclareMathOperator{\var}{var}
\newcommand{\widgraph}[2]{\includegraphics[keepaspectratio,width=#1]{#2}}
\newcommand{\widgraphr}[3]{\includegraphics[keepaspectratio,width=#1,angle=#3]{#2}}

\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf S\&DS 411 (Fall 2019) Selected Topics in Statistical Decision Theory \hfill Lecture: 6} }
       \vspace{6mm}
       \hbox to 6.28in { {\hfill #1  \hfill} }
       \vspace{6mm}
       \hbox to 6.28in { {\it Lecturer: #2 \hfill Scribe: #3} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{#1}{#1}
   \vspace*{4mm}
}


%%%%%%%
% Some commonly used notation
%%%%%%%

\def\R{{\mathbb R}}
\def\X{{\mathcal X}}
\def\Y{{\mathcal Y}}
\def\E{{\mathbb E}}
\def\sign{{\rm sign}}
\newcommand{\prob}{\ensuremath{\mathbb{P}}}

\newcommand{\percent}{$\%$}

\begin{document}


%% SOME RULES
%% - only use pdfLaTeX to compile!!
%% - for aligned equations, use align, do NOT use eqnarray
%% - for unreferenced displayed math, use \[\] and do not number them
%% - for labeling, use the following convention
%%          theorem: 			\label{thm:fermat}
%%          definition: 	\label{def:entropy}
%%          lemma: 				\label{lmm:fermat}
%%          proposition: 	\label{prop:fermat}, etc

 
%% lecture starts below
%% 1. title (short summary of the content)
%% 2. lecturer name
%% 3. scribe name (that is you) and date of lecture
%% 4. lecture number (also name the file accordingly like lec01.tex)
\lecture{Sparse Vector Estimation}{Harrison Zhou}{Cynthia Yue, Oct 2, 2019}{7}
%%%%%%%%%%%%%%%%%%%%%%%%%scribe^name^here%%^lecture number

\section{Sparse Vector Estimation}

We will look at the lower bound for sparse vector estimation and Bayesian contraction.
 
Let us consider the model, where $Z_{i} \sim \mathcal{N}(0, 1)$ and $i=1, \ldots, p$ :
\[
Y_{i} = \theta_{i} + \frac{1}{\sqrt{n}} Z_{i}.
\]

We will work under the assumption for the parameter space that
\[
\Theta = \{ \theta: \| \theta \|_0 \leq s \}.
\]

From the previous lecture, we found the risk upper bound to be:
\[
\sup \limits_{\theta \in \Theta} \mathbb{E} \| \hat\theta - \theta \|^2 \leq c \frac{s \log \frac{ep}{s}}{n}.
\]

\section{Lower Bound}

We have the following goal:
\[
\inf \limits_{\hat\theta} \sup \limits_{\theta \in \Theta} \mathbb{E} \| \hat\theta - \theta \|^2 \geq c \frac{s \log \frac{ep}{s}}{n}
\]
under the assumption $p^\epsilon \leq s \leq p^{1 - \epsilon}$ where $\epsilon$ small $> 0$.

Let us review. For $i=1, \ldots, s$,
\[
\theta_i = \begin{cases}
\frac{1}{\sqrt{n}} &  \frac{1}{2} \\
0 & \frac{1}{2}.
\end{cases}
\]

We showed that
\[
\sup \limits_{\theta \in \Theta_{sub}} \mathbb{E} \| \hat\theta - \theta \|^2 \geq c \frac{s}{n}.
\]

For the lower bound, we consider some first ideas. We could change the support to $s \log \frac{ep}{s}$ and change $\theta_i = \begin{cases}
\frac{1}{\sqrt{n}} \sqrt{\log \frac{ep}{s}} \\
0.
\end{cases}$ However, this is not in the parameter set. Instead, another way is to extend the sub-parameter space.

Let us consider the sub-parameter space $\Theta_{sub}$
\[
\theta_i = \begin{cases}
\frac{1}{\sqrt{n}} \sqrt{\log \frac{p}{s}} & i \in T \\
0 & i \not \in T.
\end{cases}
\]

We have $T \subset \{1, \ldots, p \}$ and Card($T) = s.$
There are ${p \choose s}$ ways to get $T$.

Let us consider the following facts:
\begin{enumerate}
\item \[
\max_{1 \leq i \leq p} \{ \frac{1}{\sqrt{n}} Z_i \} = (1 + O(1)) \frac{1}{\sqrt{n}}\sqrt{2 \log p}.
\]
\item \[
\begin{aligned}
| Z |_{[1]} &\geq | Z |_{[2]} \geq \ldots \geq | Z |_{[n]} \\
\frac{1}{\sqrt{n}}Z_{[s]} &= (1 + O(1)) \frac{1}{\sqrt{n}} \sqrt{2\log \frac{p}{s}}.
\end{aligned}
\]
It is impossible to identify because the signal is too weak, smaller than the noise. The signal strength is smaller than $T$, so we can't identify $T$.
\end{enumerate}

With Bayesian contraction, we can consider the prior $\pi$:
\[
\theta_i = \begin{cases}
\frac{1}{\sqrt{n}}\sqrt{\log \frac{p}{s}} & q=\frac{s}{2p} \\
0 & 1-q.
\end{cases}
\]
constrained on $\Theta$ i.i.d.

\[
\begin{aligned}
\mathbb{P} \Bigg\{ \sqrt{\mathrm{Bin}(p, \frac{s}{2p})} - \sqrt{s} \geq 0 \Bigg\} &= \mathbb{P} \Bigg\{ \sqrt{\mathrm{Bin}(p, \frac{s}{2p})} - \sqrt{\frac{s}{2}} \geq \sqrt{s} - \frac{\sqrt{s}}{2} \Bigg\} \\
&\leq \mathrm{e}^{(-c's)} \\
&\leq \mathrm{e}^{(-c p^\epsilon)} \\
&\leq \mathrm{e}^{(-c n^{\epsilon '})} \\
&\leq n^{-100}.
\end{aligned}
\]

Let us prove the lower bound:

\begin{proof}

\[
\begin{aligned}
\sup \limits_\theta \mathbb{E} \| \hat\theta_B - \theta \|^2 &\geq \int \limits_\Theta \mathbb{E} \| \hat\theta_B - \theta \|^2 d\pi / \pi(\Theta) \quad \text{where} \ \pi(\Theta) \leq 1 \\
&\geq \int \limits_\Theta \mathbb{E} \| \hat\theta_B - \theta \|^2 d\pi \\
&= \underbrace{\int \mathbb{E} \| \hat\theta_B - \theta \|^2 d\pi}_\text{dominating term} - \underbrace{\int \limits_{\Theta^c} \mathbb{E} \| \hat\theta_B - \theta \|^2 d\pi}_\text{low-order term}. \\
\int \limits_{\Theta^c} \mathbb{E} \| \hat\theta_B - \theta \|^2 d\pi &\leq 4 \log \frac{p}{s} \pi (\Theta^c) = O(n^{-2}) \\
\text{where} \ \| \theta \|^2 &\leq p \Bigg( \frac{1}{\sqrt12{n}} \sqrt{\log \frac{p}{s}} \Bigg)^2 = \frac{p}{n} \log \frac{p}{s}. \\
\int \mathbb{E} \| \hat\theta_B - \theta \|^2 d\pi &\geq \int \mathbb{E} \| \mathbb{E} (\theta | Y) - \theta \|^2 d\pi. \\
\theta_i &= \begin{cases}
\underbrace{\frac{1}{\sqrt{n}} \sqrt{\log \frac{p}{s}}}_\text{a} & q= \frac{s}{2p} \\
0 & 1-q.
\end{cases} \\
\mathbb{P} (\theta_i = 0 | Y_i) &= \frac{(1-q) \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{(Y_i - 0)^2}{2(\frac{1}{\sqrt{n}})^2}n}}{(1-q) \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{(Y_i - 0)^2}{2}n} + q \frac{1}{\sqrt{2\pi}} \mathrm{e}^{-\frac{(Y_i - a)^2}{2}n}} \\
&= \frac{1-q}{(1-q) + q \mathrm{e}^{Y_i an} \mathrm{e}^{-\frac{na^2}{2}}}. \\
\mathbb{P}(\theta_i = a | Y_i) &= 1 - \mathbb{P} (\theta_i = 0 | Y_i).
\end{aligned}
\]

Therefore, we have

\[
\int \mathbb{E} \| \hat\theta_B - \theta \|^2 d\pi = \sum \limits_{i=1}^p \bigg[ q \bigg( \mathbb{E} (\theta_i | Y) - a \bigg)^2 + (1-q) \bigg( \mathbb{E} (\theta_i | Y) - 0 \bigg)^2 \bigg].
\]

For the posterior norm, we have
\[
\begin{aligned}
\mathbb{E} (\theta_i | Y_i) &= \mathbb{P} (\theta_i = a | Y_i) a + \mathbb{P} (\theta_i = 0 | Y_i) 0 \\
&= \mathbb{P} (\theta_i = a | Y_i) a.
\end{aligned}
\]

If we plug this in, we get the desired lower bound:
\[
\begin{aligned}
\int \mathbb{E} \| \hat\theta_B - \theta \|^2 d\pi &= \sum \limits_{i=1}^p \bigg[ q \bigg( \mathbb{E} (\theta_i | Y) - a \bigg)^2 + (1-q) \bigg( \mathbb{E} (\theta_i | Y) - 0 \bigg)^2 \bigg] \\
&\geq q \sum \limits_{i=1}^p \mathbb{E}_{Y_i | \theta_i = a} \bigg( \mathbb{E} (\theta_i | Y_i) - a \bigg) ^2 \\
&= q \sum \limits_{i=1}^p \mathbb{E}_{Y_i | \theta_i = a} \bigg( \mathbb{P} (\theta_i = a | Y_i)a - a \bigg)^2 \\
&= q a^2 \sum \limits_{i=1}^p \underbrace{\mathbb{E}_{Y_i | \theta_i = a} \bigg( \mathbb{P}(\theta_i = 0 | Y_i) \bigg)^2}_{\leq \text{C}} \\
&\geq qa^2pC.
\end{aligned}
\]

\end{proof}

Now,
\[
\begin{aligned}
Y_i &= a + \frac{1}{\sqrt{n}} Z_i. \\
\mathbb{P} (\theta_i = 0 | Y_i) &= \frac{1-q}{1-q + \frac{s}{2p} \mathrm{e}^{(a + \frac{1}{\sqrt{n}} Z_i)an - \frac{1}{2}na^2}} \\
&= \frac{1-q}{1-q + \frac{1}{2} \mathrm{e}^{\sqrt{\log \frac{p}{s}}Z_i - \frac{1}{2} \log \frac{p}{s}}} \\
&\to 1 \ \text{as} \ p \to +\infty
\end{aligned}
\]
so we can find constant $C_1$.

Another way to do this is to use Fano's lemma.

For Fano's lemma, if we have the joint distribution $Y= (Y_1, \ldots, Y_p)$, we can use the lemma to show that if we have $\{ \theta_0, \theta_1, \ldots, \theta^{M^*} \} \subset \Theta$ and if $d(\theta^i, \theta^j) \geq 2\epsilon$ where we choose $\epsilon = \sqrt{\frac{s \log \frac{p}{s}}{n}}\gamma$, and $\frac{1}{M^*} \sum \limits_{k=1}^{M^*}k(P_{\theta^k} | P_{\theta^0}) \leq c \log M*$ for some $0<c<\frac{1}{8}$, then for any $\hat\theta$,
\[
\sup \limits_{d \leq k \leq M*} \mathbb{E} d^2 (\hat\theta, \theta_k) \geq \epsilon^2 \underbrace{\frac{\sqrt{M*}}{1+\sqrt{M*}} (1 - 2c - \sqrt{\frac{2c}{\log M*}})}_{\geq \frac{1}{2}}
\]

For $\Theta_{sub}$,
\[
\begin{aligned}
\theta^0 &= 0 \\
\theta_i &= \begin{cases}
\sqrt{\log \frac{p}{s} \gamma} / \sqrt{n} & i \in T \\
0 & i \not \in T
\end{cases} \\
T &: {p \choose s} \ \text{and Card}(T)=s
\end{aligned}
\]

\begin{claim}
By Varshomov-Gilbert Lemma, we can find $\theta^1, \ldots, \theta^{M*}$ with $\log M^* \geq c_1 s \log \frac{p}{s}$ for some $c_1$ s.t.
\[
d(\theta^i, \theta^j) \geq 2 \sqrt{\frac{s \log \frac{p}{s}}{n} \frac{1}{16}}.
\]
\end{claim}

\nocite{*}
\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
